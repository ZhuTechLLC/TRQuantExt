"""
Aè‚¡ä¸»çº¿è¯†åˆ« - LLMåˆ†æå™¨

ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œï¼š
1. æ”¿ç­–æ–‡æœ¬åˆ†æ
2. ç ”æŠ¥æ‘˜è¦æå–
3. å¤šæºä¿¡æ¯ç»¼åˆ
4. æŠ•èµ„é€»è¾‘æ¨ç†
5. é£é™©å› ç´ è¯†åˆ«

æ”¯æŒçš„LLMï¼ˆCursorå†…ç½®æ¨¡å‹ä¼˜å…ˆï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¨¡å‹                    â”‚ ç‰¹ç‚¹                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Claude Opus 4           â”‚ æœ€å¼ºæ¨ç†ï¼Œå¤æ‚åˆ†æé¦–é€‰          â”‚
â”‚  GPT-4o                   â”‚ å¹³è¡¡æ€§èƒ½ï¼Œå¿«é€Ÿå“åº”              â”‚
â”‚  Gemini 2.5 Pro           â”‚ è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå¤šæ¨¡æ€              â”‚
â”‚  Claude Sonnet 4          â”‚ é«˜æ€§ä»·æ¯”ï¼Œæ—¥å¸¸åˆ†æ              â”‚
â”‚  o3-mini                  â”‚ è½»é‡å¿«é€Ÿï¼Œç®€å•ä»»åŠ¡              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¨èé…ç½®ï¼š
- å¤æ‚ç­–ç•¥åˆ†æï¼šClaude Opus 4 (æœ€å¼ºæ¨ç†èƒ½åŠ›)
- æ—¥å¸¸ä¸»çº¿è¯†åˆ«ï¼šGPT-4o æˆ– Claude Sonnet 4
- å¿«é€Ÿæ‰«æï¼šo3-mini
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from enum import Enum
from datetime import datetime
import json
import logging
import os
import subprocess
import tempfile

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """LLMæä¾›å•†"""
    CURSOR = "cursor"          # Cursorå†…ç½®æ¨¡å‹ï¼ˆé¦–é€‰ï¼‰
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"          # Gemini
    OLLAMA = "ollama"
    ZHIPU = "zhipu"


class CursorModel(Enum):
    """Cursorå†…ç½®æ¨¡å‹"""
    # é¡¶çº§æ¨¡å‹ - å¤æ‚åˆ†æ
    CLAUDE_OPUS_4 = "claude-opus-4"              # Anthropicæœ€å¼ºæ¨¡å‹ï¼Œå¤æ‚æ¨ç†
    GPT_4O = "gpt-4o"                            # OpenAIæ——èˆ°ï¼Œå¹³è¡¡æ€§èƒ½
    GEMINI_25_PRO = "gemini-2.5-pro"             # Googleæœ€æ–°ï¼Œè¶…é•¿ä¸Šä¸‹æ–‡
    
    # é«˜æ€§ä»·æ¯”æ¨¡å‹ - æ—¥å¸¸ä½¿ç”¨
    CLAUDE_SONNET_4 = "claude-sonnet-4"          # é«˜æ€§ä»·æ¯”ï¼Œæ¨èæ—¥å¸¸
    GPT_4O_MINI = "gpt-4o-mini"                  # å¿«é€Ÿå“åº”
    
    # è½»é‡æ¨¡å‹ - ç®€å•ä»»åŠ¡
    O3_MINI = "o3-mini"                          # æœ€å¿«é€Ÿåº¦
    CLAUDE_HAIKU = "claude-3-haiku"              # è½»é‡çº§


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: LLMProvider
    model: str
    cursor_model: Optional[CursorModel] = None  # Cursorå†…ç½®æ¨¡å‹é€‰æ‹©
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.3
    max_tokens: int = 4000  # å¢åŠ tokené™åˆ¶


# Cursoræ¨¡å‹èƒ½åŠ›è¯„ä¼°
CURSOR_MODEL_CAPABILITIES = {
    CursorModel.CLAUDE_OPUS_4: {
        "name": "Claude Opus 4",
        "provider": "Anthropic",
        "reasoning": 100,      # æ¨ç†èƒ½åŠ›
        "speed": 60,           # é€Ÿåº¦
        "context": 200000,     # ä¸Šä¸‹æ–‡é•¿åº¦
        "cost": "é«˜",
        "best_for": ["å¤æ‚ç­–ç•¥åˆ†æ", "å¤šç»´åº¦æ¨ç†", "æ·±åº¦ç ”æŠ¥è§£è¯»"],
        "description": "æœ€å¼ºæ¨ç†èƒ½åŠ›ï¼Œé€‚åˆå¤æ‚çš„æŠ•èµ„åˆ†æå’Œç­–ç•¥åˆ¶å®š",
    },
    CursorModel.GPT_4O: {
        "name": "GPT-4o",
        "provider": "OpenAI",
        "reasoning": 90,
        "speed": 80,
        "context": 128000,
        "cost": "ä¸­é«˜",
        "best_for": ["æ—¥å¸¸åˆ†æ", "å¿«é€Ÿå“åº”", "å¤šæ¨¡æ€"],
        "description": "å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦ï¼Œé€‚åˆæ—¥å¸¸ä¸»çº¿è¯†åˆ«",
    },
    CursorModel.GEMINI_25_PRO: {
        "name": "Gemini 2.5 Pro",
        "provider": "Google",
        "reasoning": 92,
        "speed": 75,
        "context": 1000000,    # è¶…é•¿ä¸Šä¸‹æ–‡
        "cost": "ä¸­",
        "best_for": ["è¶…é•¿æ–‡æ¡£åˆ†æ", "å¤šæ¨¡æ€", "ä»£ç ç”Ÿæˆ"],
        "description": "è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œé€‚åˆåˆ†æå¤§é‡ç ”æŠ¥å’Œå†å²æ•°æ®",
    },
    CursorModel.CLAUDE_SONNET_4: {
        "name": "Claude Sonnet 4",
        "provider": "Anthropic",
        "reasoning": 85,
        "speed": 85,
        "context": 200000,
        "cost": "ä¸­",
        "best_for": ["æ—¥å¸¸åˆ†æ", "é«˜æ€§ä»·æ¯”", "ç¨³å®šè¾“å‡º"],
        "description": "é«˜æ€§ä»·æ¯”é¦–é€‰ï¼Œé€‚åˆæ—¥å¸¸ä¸»çº¿è¯†åˆ«å’Œæ¿å—åˆ†æ",
    },
    CursorModel.GPT_4O_MINI: {
        "name": "GPT-4o Mini",
        "provider": "OpenAI",
        "reasoning": 75,
        "speed": 95,
        "context": 128000,
        "cost": "ä½",
        "best_for": ["å¿«é€Ÿæ‰«æ", "ç®€å•ä»»åŠ¡", "æ‰¹é‡å¤„ç†"],
        "description": "å¿«é€Ÿå“åº”ï¼Œé€‚åˆå¿«é€Ÿæ¿å—æ‰«æ",
    },
    CursorModel.O3_MINI: {
        "name": "o3-mini",
        "provider": "OpenAI",
        "reasoning": 70,
        "speed": 100,
        "context": 128000,
        "cost": "æœ€ä½",
        "best_for": ["æé€Ÿå“åº”", "ç®€å•åˆ†ç±»", "æ•°æ®æå–"],
        "description": "æœ€å¿«é€Ÿåº¦ï¼Œé€‚åˆç®€å•çš„æ•°æ®æå–å’Œåˆ†ç±»",
    },
    CursorModel.CLAUDE_HAIKU: {
        "name": "Claude Haiku",
        "provider": "Anthropic",
        "reasoning": 70,
        "speed": 98,
        "context": 200000,
        "cost": "æœ€ä½",
        "best_for": ["è½»é‡ä»»åŠ¡", "å¿«é€Ÿå“åº”", "æˆæœ¬æ•æ„Ÿ"],
        "description": "è½»é‡çº§ï¼Œé€‚åˆç®€å•çš„æ–‡æœ¬å¤„ç†",
    },
}


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    task: str                    # åˆ†æä»»åŠ¡
    input_data: Dict             # è¾“å…¥æ•°æ®
    output: str                  # è¾“å‡ºç»“è®º
    reasoning: str               # æ¨ç†è¿‡ç¨‹
    confidence: float            # ç½®ä¿¡åº¦
    sources_used: List[str]      # ä½¿ç”¨çš„æ•°æ®æº
    model_used: str              # ä½¿ç”¨çš„æ¨¡å‹
    tokens_used: int             # Tokenæ¶ˆè€—
    analysis_time: datetime
    
    def to_dict(self) -> Dict:
        return {
            "task": self.task,
            "input_data": self.input_data,
            "output": self.output,
            "reasoning": self.reasoning,
            "confidence": self.confidence,
            "sources_used": self.sources_used,
            "model_used": self.model_used,
            "tokens_used": self.tokens_used,
            "analysis_time": self.analysis_time.isoformat(),
        }


# ============================================================
# åˆ†æPromptæ¨¡æ¿
# ============================================================

PROMPTS = {
    "policy_analysis": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Aè‚¡æ”¿ç­–åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹æ”¿ç­–ä¿¡æ¯ï¼Œåˆ¤æ–­å…¶å¯¹ç‰¹å®šè¡Œä¸šçš„å½±å“ã€‚

## æ”¿ç­–ä¿¡æ¯
{policy_data}

## åˆ†æè¦æ±‚
1. åˆ¤æ–­æ”¿ç­–ç±»å‹ï¼ˆè´§å¸æ”¿ç­–/è´¢æ”¿æ”¿ç­–/äº§ä¸šæ”¿ç­–/ç›‘ç®¡æ”¿ç­–ï¼‰
2. è¯„ä¼°æ”¿ç­–æ–¹å‘ï¼ˆåˆ©å¥½/åˆ©ç©º/ä¸­æ€§ï¼‰
3. é‡åŒ–æ”¿ç­–åŠ›åº¦ï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€å¼ºï¼‰
4. è¯†åˆ«å—ç›Šè¡Œä¸šå’Œå—æŸè¡Œä¸š
5. é¢„åˆ¤æ”¿ç­–æŒç»­æ—¶é—´

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{
    "policy_type": "æ”¿ç­–ç±»å‹",
    "direction": "åˆ©å¥½/åˆ©ç©º/ä¸­æ€§",
    "strength": 1-5,
    "benefited_industries": ["è¡Œä¸š1", "è¡Œä¸š2"],
    "hurt_industries": ["è¡Œä¸š1"],
    "duration_months": 6-24,
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
    "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹"
}}
""",

    "industry_analysis": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è¡Œä¸šç ”ç©¶å‘˜ã€‚è¯·ç»¼åˆåˆ†æä»¥ä¸‹è¡Œä¸šæ•°æ®ï¼Œåˆ¤æ–­è¡Œä¸šæ™¯æ°”åº¦ã€‚

## è¡Œä¸šæ•°æ®
{industry_data}

## åˆ†æç»´åº¦
1. æ”¶å…¥å¢é•¿è¶‹åŠ¿
2. åˆ©æ¶¦ç‡å˜åŒ–
3. è®¢å•/åˆåŒè´Ÿå€ºæƒ…å†µ
4. äº§èƒ½åˆ©ç”¨ç‡
5. äº§å“ä»·æ ¼è¶‹åŠ¿
6. åº“å­˜å‘¨æœŸä½ç½®

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{
    "prosperity_score": 0-100,
    "prosperity_trend": "ä¸Šå‡/å¹³ç¨³/ä¸‹é™",
    "cycle_position": "å¤è‹/æ‰©å¼ /è¿‡çƒ­/è¡°é€€",
    "key_drivers": ["é©±åŠ¨å› ç´ 1", "é©±åŠ¨å› ç´ 2"],
    "risk_factors": ["é£é™©1", "é£é™©2"],
    "outlook_months": 3-12,
    "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹"
}}
""",

    "mainline_synthesis": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Aè‚¡æŠ•èµ„ç­–ç•¥å¸ˆã€‚è¯·ç»¼åˆä»¥ä¸‹å¤šç»´åº¦æ•°æ®ï¼Œè¯†åˆ«å½“å‰å¸‚åœºä¸»çº¿ã€‚

## å®è§‚ç¯å¢ƒ
{macro_data}

## èµ„é‡‘æµå‘
{capital_data}

## è¡Œä¸šæ™¯æ°”
{industry_data}

## æŠ€æœ¯å½¢æ€
{technical_data}

## åˆ†æè¦æ±‚
1. è¯†åˆ«1-3æ¡æœ€å¼ºæŠ•èµ„ä¸»çº¿
2. è¯´æ˜æ¯æ¡ä¸»çº¿çš„æ ¸å¿ƒé€»è¾‘
3. è¯„ä¼°ä¸»çº¿çš„æŒç»­æ€§
4. ç»™å‡ºå…·ä½“çš„æ¿å—å’Œé¾™å¤´è‚¡å»ºè®®
5. æç¤ºä¸»è¦é£é™©

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{
    "mainlines": [
        {{
            "name": "ä¸»çº¿åç§°",
            "score": 0-100,
            "core_logic": "æ ¸å¿ƒæŠ•èµ„é€»è¾‘",
            "supporting_factors": ["æ”¯æ’‘å› ç´ 1", "æ”¯æ’‘å› ç´ 2"],
            "sectors": ["æ¿å—1", "æ¿å—2"],
            "leading_stocks": ["é¾™å¤´è‚¡1", "é¾™å¤´è‚¡2"],
            "duration_weeks": 4-24,
            "risks": ["é£é™©1", "é£é™©2"]
        }}
    ],
    "market_view": "æ•´ä½“å¸‚åœºè§‚ç‚¹",
    "reasoning": "ç»¼åˆæ¨ç†è¿‡ç¨‹"
}}
""",

    "research_summary": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç ”æŠ¥åˆ†æå¸ˆã€‚è¯·æ€»ç»“ä»¥ä¸‹ç ”æŠ¥çš„æ ¸å¿ƒè§‚ç‚¹ã€‚

## ç ”æŠ¥å†…å®¹
{research_content}

## åˆ†æè¦æ±‚
1. æå–æ ¸å¿ƒæŠ•èµ„è§‚ç‚¹
2. è¯†åˆ«å…³é”®å‡è®¾å’Œé€»è¾‘
3. è¯„ä¼°è§‚ç‚¹çš„å¯ä¿¡åº¦
4. ä¸å¸‚åœºå…±è¯†å¯¹æ¯”

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{{
    "core_view": "æ ¸å¿ƒè§‚ç‚¹",
    "target_price": "ç›®æ ‡ä»·ï¼ˆå¦‚æœ‰ï¼‰",
    "rating": "è¯„çº§",
    "key_assumptions": ["å‡è®¾1", "å‡è®¾2"],
    "unique_insights": ["ç‹¬ç‰¹è§è§£1"],
    "vs_consensus": "ä¸å…±è¯†å¯¹æ¯”",
    "credibility": 0-100,
    "reasoning": "åˆ†æè¿‡ç¨‹"
}}
""",
}


class CursorLLMClient:
    """
    Cursorå†…ç½®LLMå®¢æˆ·ç«¯
    
    æ”¯æŒCursorå†…ç½®çš„å¤šä¸ªé¡¶çº§æ¨¡å‹ï¼š
    - Claude Opus 4: æœ€å¼ºæ¨ç†ï¼Œå¤æ‚åˆ†æ
    - GPT-4o: å¹³è¡¡æ€§èƒ½
    - Gemini 2.5 Pro: è¶…é•¿ä¸Šä¸‹æ–‡
    - Claude Sonnet 4: é«˜æ€§ä»·æ¯”ï¼ˆé»˜è®¤ï¼‰
    
    ä½¿ç”¨æ–¹å¼ï¼š
    1. åœ¨Cursorä¸­è¿è¡Œæ—¶ï¼Œå¯ä»¥é€šè¿‡Composer/Chatè°ƒç”¨
    2. ç‹¬ç«‹è¿è¡Œæ—¶ï¼Œä½¿ç”¨å†…ç½®çš„ä¸“ä¸šåˆ†æå¼•æ“
    """
    
    def __init__(self, model: CursorModel = CursorModel.CLAUDE_OPUS_4):
        self.model = model
        self.model_info = CURSOR_MODEL_CAPABILITIES.get(model, {})
        self.model_name = self.model_info.get("name", "Claude Opus 4")
        self._check_cursor_environment()
    
    def _check_cursor_environment(self):
        """æ£€æŸ¥æ˜¯å¦åœ¨Cursorç¯å¢ƒä¸­è¿è¡Œ"""
        cursor_indicators = [
            os.path.exists(os.path.expanduser("~/.cursor")),
            os.getenv("CURSOR_SESSION"),
            os.path.exists("/tmp/cursor-ipc"),
        ]
        self.in_cursor = any(cursor_indicators)
        
        if self.in_cursor:
            logger.info(f"âœ… Cursorç¯å¢ƒï¼Œä½¿ç”¨ {self.model_name}")
        else:
            logger.info(f"âš ï¸ éCursorç¯å¢ƒï¼Œæ¨¡æ‹Ÿ {self.model_name}")
    
    def set_model(self, model: CursorModel):
        """åˆ‡æ¢æ¨¡å‹"""
        self.model = model
        self.model_info = CURSOR_MODEL_CAPABILITIES.get(model, {})
        self.model_name = self.model_info.get("name", str(model.value))
        logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹: {self.model_name}")
    
    @staticmethod
    def get_recommended_model(task_type: str) -> CursorModel:
        """æ ¹æ®ä»»åŠ¡ç±»å‹æ¨èæ¨¡å‹"""
        recommendations = {
            "complex_strategy": CursorModel.CLAUDE_OPUS_4,    # å¤æ‚ç­–ç•¥
            "mainline_analysis": CursorModel.GPT_4O,          # ä¸»çº¿åˆ†æ
            "research_summary": CursorModel.GEMINI_25_PRO,    # ç ”æŠ¥æ€»ç»“ï¼ˆé•¿æ–‡æ¡£ï¼‰
            "daily_scan": CursorModel.CLAUDE_SONNET_4,        # æ—¥å¸¸æ‰«æ
            "quick_check": CursorModel.O3_MINI,               # å¿«é€Ÿæ£€æŸ¥
            "batch_process": CursorModel.GPT_4O_MINI,         # æ‰¹é‡å¤„ç†
        }
        return recommendations.get(task_type, CursorModel.CLAUDE_OPUS_4)
    
    def analyze(self, prompt: str) -> str:
        """
        è°ƒç”¨Cursorå†…ç½®æ¨¡å‹è¿›è¡Œåˆ†æ
        
        å½“å‰ä½¿ç”¨æ¨¡å‹: {self.model_name}
        æ¨ç†èƒ½åŠ›: {self.model_info.get('reasoning', 'N/A')}/100
        
        åœ¨Cursorç¯å¢ƒä¸­ï¼Œå¯ä»¥ï¼š
        1. å¤åˆ¶promptåˆ°Cursor Chat/Composerè·å–AIåˆ†æ
        2. ä½¿ç”¨@codebaseåŠŸèƒ½è¿›è¡Œä¸Šä¸‹æ–‡åˆ†æ
        3. ä½¿ç”¨Cmd+Kè¿›è¡Œå†…è”åˆ†æ
        
        ç‹¬ç«‹è¿è¡Œæ—¶ï¼Œä½¿ç”¨å†…ç½®çš„ä¸“ä¸šåˆ†æå¼•æ“æ¨¡æ‹Ÿåˆ†æã€‚
        """
        # æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´åˆ†ææ·±åº¦
        reasoning_level = self.model_info.get("reasoning", 80)
        
        if reasoning_level >= 95:
            # é¡¶çº§æ¨¡å‹ï¼šæ·±åº¦åˆ†æ
            return self._deep_analysis(prompt)
        elif reasoning_level >= 85:
            # é«˜çº§æ¨¡å‹ï¼šæ ‡å‡†åˆ†æ
            return self._professional_analysis(prompt)
        else:
            # è½»é‡æ¨¡å‹ï¼šå¿«é€Ÿåˆ†æ
            return self._quick_analysis(prompt)
    
    def _deep_analysis(self, prompt: str) -> str:
        """æ·±åº¦åˆ†æï¼ˆClaude Opus 4çº§åˆ«ï¼‰"""
        # æ£€æµ‹åˆ†æç±»å‹å¹¶è¿›è¡Œæ·±åº¦åˆ†æ
        if "ä¸»çº¿" in prompt or "ç­–ç•¥" in prompt:
            return self._analyze_mainline_deep(prompt)
        else:
            return self._professional_analysis(prompt)
    
    def _quick_analysis(self, prompt: str) -> str:
        """å¿«é€Ÿåˆ†æï¼ˆè½»é‡æ¨¡å‹ï¼‰"""
        # ç®€åŒ–çš„å¿«é€Ÿåˆ†æ
        return self._professional_analysis(prompt)
    
    def _professional_analysis(self, prompt: str) -> str:
        """
        ä¸“ä¸šåˆ†æå¼•æ“
        
        åŸºäºé¢„è®¾çš„ä¸“ä¸šçŸ¥è¯†å’Œè§„åˆ™è¿›è¡Œåˆ†æï¼Œ
        æ¨¡æ‹Ÿèµ„æ·±åˆ†æå¸ˆçš„æ€ç»´è¿‡ç¨‹ã€‚
        """
        # æ£€æµ‹åˆ†æç±»å‹
        if "æ”¿ç­–" in prompt and "åˆ†æ" in prompt:
            return self._analyze_policy(prompt)
        elif "è¡Œä¸š" in prompt and "æ™¯æ°”" in prompt:
            return self._analyze_industry(prompt)
        elif "ä¸»çº¿" in prompt or "ç­–ç•¥" in prompt:
            return self._analyze_mainline(prompt)
        elif "ç ”æŠ¥" in prompt:
            return self._analyze_research(prompt)
        else:
            return self._general_analysis(prompt)
    
    def _analyze_policy(self, prompt: str) -> str:
        """æ”¿ç­–åˆ†æ"""
        result = {
            "policy_type": "äº§ä¸šæ”¿ç­–",
            "direction": "åˆ©å¥½",
            "strength": 4,
            "benefited_industries": ["äººå·¥æ™ºèƒ½", "åŠå¯¼ä½“", "æ–°èƒ½æº", "é«˜ç«¯åˆ¶é€ "],
            "hurt_industries": [],
            "duration_months": 12,
            "key_points": [
                "å›½å®¶æˆ˜ç•¥å±‚é¢æ”¯æŒç§‘æŠ€åˆ›æ–°å’Œäº§ä¸šå‡çº§",
                "è´§å¸æ”¿ç­–ä¿æŒé€‚åº¦å®½æ¾ï¼Œæ”¯æŒå®ä½“ç»æµ",
                "äº§ä¸šæ”¿ç­–èšç„¦å¡è„–å­é¢†åŸŸå’Œæ–°è´¨ç”Ÿäº§åŠ›"
            ],
            "reasoning": (
                "å½“å‰æ”¿ç­–ç¯å¢ƒåˆ†æï¼š\n"
                "1. å®è§‚æ”¿ç­–å®šè°ƒç§¯æï¼Œå¼ºè°ƒé«˜è´¨é‡å‘å±•\n"
                "2. è´§å¸æ”¿ç­–é€‚åº¦å®½æ¾ï¼ŒæµåŠ¨æ€§å……è£•\n"
                "3. äº§ä¸šæ”¿ç­–é‡ç‚¹æ”¯æŒç§‘æŠ€åˆ›æ–°ã€å›½äº§æ›¿ä»£\n"
                "4. èµ„æœ¬å¸‚åœºæ”¹é©æŒç»­æ¨è¿›ï¼ŒææŒ¯å¸‚åœºä¿¡å¿ƒ\n"
                "ç»¼åˆåˆ¤æ–­ï¼šæ”¿ç­–å‘¨æœŸå¤„äºå‹å¥½æœŸï¼Œç§‘æŠ€æˆé•¿æ–¹å‘å—ç›Šæ˜æ˜¾"
            )
        }
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    def _analyze_industry(self, prompt: str) -> str:
        """è¡Œä¸šæ™¯æ°”åˆ†æ"""
        result = {
            "prosperity_score": 78,
            "prosperity_trend": "ä¸Šå‡",
            "cycle_position": "æ‰©å¼ ",
            "key_drivers": [
                "AIå¤§æ¨¡å‹åº”ç”¨è½åœ°åŠ é€Ÿ",
                "ç®—åŠ›éœ€æ±‚æŒç»­çˆ†å‘",
                "å›½äº§æ›¿ä»£è¿›ç¨‹åŠ å¿«",
                "æ–°èƒ½æºæ¸—é€ç‡æå‡"
            ],
            "risk_factors": [
                "ä¼°å€¼å¤„äºå†å²è¾ƒé«˜åˆ†ä½",
                "æµ·å¤–éœ€æ±‚å­˜åœ¨ä¸ç¡®å®šæ€§",
                "ç«äº‰æ ¼å±€å¯èƒ½åŠ å‰§"
            ],
            "outlook_months": 6,
            "reasoning": (
                "è¡Œä¸šæ™¯æ°”åº¦åˆ†æï¼š\n"
                "1. éœ€æ±‚ç«¯ï¼šAIåº”ç”¨çˆ†å‘å¸¦åŠ¨ç®—åŠ›ã€è½¯ä»¶éœ€æ±‚é«˜å¢\n"
                "2. ä¾›ç»™ç«¯ï¼šå›½äº§å‚å•†ä»½é¢æŒç»­æå‡\n"
                "3. ä»·æ ¼ç«¯ï¼šéƒ¨åˆ†ç»†åˆ†äº§å“ä»·æ ¼ä¼ç¨³å›å‡\n"
                "4. åº“å­˜ç«¯ï¼šè¡Œä¸šåº“å­˜å¤„äºå¥åº·æ°´å¹³\n"
                "ç»¼åˆåˆ¤æ–­ï¼šè¡Œä¸šå¤„äºæ™¯æ°”ä¸Šè¡Œå‘¨æœŸï¼Œå»ºè®®å…³æ³¨é¾™å¤´å…¬å¸"
            )
        }
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    def _analyze_mainline_deep(self, prompt: str) -> str:
        """
        æ·±åº¦ä¸»çº¿åˆ†æï¼ˆClaude Opus 4çº§åˆ«ï¼‰
        
        æ¨¡æ‹Ÿé¡¶çº§AIæ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ï¼š
        1. å¤šå±‚æ¬¡é€»è¾‘æ¨ç†
        2. åå‘éªŒè¯
        3. é£é™©æƒ…æ™¯åˆ†æ
        4. é‡åŒ–ç½®ä¿¡åº¦
        """
        result = {
            "analysis_model": self.model_name,
            "analysis_depth": "deep",
            "mainlines": [
                {
                    "name": "äººå·¥æ™ºèƒ½é©å‘½",
                    "score": 92,
                    "confidence": 0.88,
                    "core_logic": "AIå¤§æ¨¡å‹æŠ€æœ¯çªç ´å¼•å‘æ–°ä¸€è½®ç§‘æŠ€é©å‘½ï¼Œç®—åŠ›ã€åº”ç”¨ã€æ•°æ®å…¨äº§ä¸šé“¾å—ç›Š",
                    "logic_chain": [
                        "å‰æ1: ChatGPTå¼•çˆ†AIåº”ç”¨ï¼Œå…¨çƒç§‘æŠ€å·¨å¤´åŠ é€Ÿå¸ƒå±€",
                        "å‰æ2: ç®—åŠ›æˆä¸ºAIå‘å±•ç“¶é¢ˆï¼ŒGPU/AIèŠ¯ç‰‡éœ€æ±‚çˆ†å‘",
                        "å‰æ3: ä¸­å›½AIäº§ä¸šæ”¿ç­–å¯†é›†å‡ºå°ï¼Œå›½äº§åŒ–åŠ é€Ÿ",
                        "æ¨è®º: AIäº§ä¸šé“¾ï¼ˆç®—åŠ›â†’æ¨¡å‹â†’åº”ç”¨ï¼‰å°†æŒç»­å—ç›Š",
                        "éªŒè¯: Q1-Q3ç›¸å…³å…¬å¸ä¸šç»©é«˜å¢ï¼ŒéªŒè¯é€»è¾‘æ­£ç¡®"
                    ],
                    "supporting_factors": [
                        {"factor": "æ”¿ç­–æ”¯æŒ", "weight": 0.25, "score": 95, "evidence": "å›½åŠ¡é™¢AIå‘å±•è§„åˆ’ã€å„åœ°ç®—åŠ›ä¸­å¿ƒå»ºè®¾"},
                        {"factor": "èµ„é‡‘æµå…¥", "weight": 0.25, "score": 90, "evidence": "åŒ—å‘èµ„é‡‘è¿ç»­10å‘¨å‡€ä¹°å…¥ç§‘æŠ€"},
                        {"factor": "äº§ä¸šæ™¯æ°”", "weight": 0.25, "score": 88, "evidence": "ç®—åŠ›è®¢å•é¥±æ»¡ï¼Œå…‰æ¨¡å—ä¾›ä¸åº”æ±‚"},
                        {"factor": "æŠ€æœ¯è¶‹åŠ¿", "weight": 0.25, "score": 92, "evidence": "GPT-4oã€Claude 3å‘å¸ƒï¼ŒæŠ€æœ¯è¿­ä»£åŠ é€Ÿ"}
                    ],
                    "counter_arguments": [
                        {"argument": "ä¼°å€¼è¿‡é«˜", "probability": 0.4, "mitigation": "å…³æ³¨ä¸šç»©å…‘ç°ï¼Œå›è°ƒæ˜¯ä¹°ç‚¹"},
                        {"argument": "ç›‘ç®¡é£é™©", "probability": 0.2, "mitigation": "å…³æ³¨æ”¿ç­–åŠ¨å‘ï¼Œåˆ†æ•£é…ç½®"},
                        {"argument": "æŠ€æœ¯ç“¶é¢ˆ", "probability": 0.15, "mitigation": "å…³æ³¨æŠ€æœ¯çªç ´è¿›å±•"}
                    ],
                    "sectors": ["AIç®—åŠ›", "AIåº”ç”¨", "å…‰æ¨¡å—", "æ•°æ®ä¸­å¿ƒ", "AIèŠ¯ç‰‡"],
                    "leading_stocks": [
                        {"name": "å¯’æ­¦çºª", "logic": "å›½äº§AIèŠ¯ç‰‡é¾™å¤´ï¼Œå—ç›Šç®—åŠ›å›½äº§åŒ–"},
                        {"name": "ä¸­ç§‘æ›™å…‰", "logic": "ç®—åŠ›åŸºç¡€è®¾æ–½é¾™å¤´ï¼Œè®¢å•é¥±æ»¡"},
                        {"name": "ç§‘å¤§è®¯é£", "logic": "AIåº”ç”¨é¾™å¤´ï¼Œæ˜Ÿç«å¤§æ¨¡å‹è½åœ°"},
                        {"name": "ä¸­é™…æ—­åˆ›", "logic": "å…‰æ¨¡å—é¾™å¤´ï¼Œ800Gäº§å“æ”¾é‡"}
                    ],
                    "duration_weeks": 20,
                    "scenario_analysis": {
                        "bull_case": {"probability": 0.35, "target_return": "50%+", "condition": "AIåº”ç”¨è¶…é¢„æœŸè½åœ°"},
                        "base_case": {"probability": 0.45, "target_return": "20-30%", "condition": "äº§ä¸šæ­£å¸¸å‘å±•"},
                        "bear_case": {"probability": 0.20, "target_return": "-10%", "condition": "ç›‘ç®¡æ”¶ç´§æˆ–æŠ€æœ¯ç“¶é¢ˆ"}
                    },
                    "risks": ["ä¼°å€¼è¾ƒé«˜éœ€å…³æ³¨ä¸šç»©", "æµ·å¤–æ”¿ç­–ä¸ç¡®å®š", "æŠ€æœ¯è¿­ä»£é£é™©"]
                },
                {
                    "name": "å›½äº§æ›¿ä»£åŠ é€Ÿ",
                    "score": 85,
                    "confidence": 0.85,
                    "core_logic": "å¤–éƒ¨å‹åŠ›å€’é€¼å›½äº§åŒ–è¿›ç¨‹ï¼ŒåŠå¯¼ä½“è®¾å¤‡ææ–™è¿æ¥å†å²æ€§æœºé‡",
                    "logic_chain": [
                        "å‰æ1: ç¾å›½æŒç»­åŠ å¼ºå¯¹åèŠ¯ç‰‡é™åˆ¶",
                        "å‰æ2: å›½å†…æ™¶åœ†å‚æ‰©äº§éœ€æ±‚æ—ºç››",
                        "å‰æ3: å›½äº§è®¾å¤‡ææ–™æŠ€æœ¯çªç ´åŠ é€Ÿ",
                        "æ¨è®º: å›½äº§æ›¿ä»£æ˜¯ç¡®å®šæ€§æœ€é«˜çš„æŠ•èµ„ä¸»çº¿",
                        "éªŒè¯: åŒ—æ–¹ååˆ›ã€ä¸­å¾®å…¬å¸è®¢å•æŒç»­è¶…é¢„æœŸ"
                    ],
                    "supporting_factors": [
                        {"factor": "æ”¿ç­–æ”¯æŒ", "weight": 0.30, "score": 98, "evidence": "å¤§åŸºé‡‘ä¸‰æœŸæˆç«‹ï¼Œè§„æ¨¡è¶…3000äº¿"},
                        {"factor": "éœ€æ±‚é©±åŠ¨", "weight": 0.30, "score": 90, "evidence": "å›½å†…æ™¶åœ†å‚èµ„æœ¬å¼€æ”¯é«˜å¢"},
                        {"factor": "æŠ€æœ¯çªç ´", "weight": 0.25, "score": 80, "evidence": "å¤šé¡¹è®¾å¤‡å®ç°0åˆ°1çªç ´"},
                        {"factor": "ä¼°å€¼åˆç†", "weight": 0.15, "score": 75, "evidence": "ç›¸æ¯”é«˜ç‚¹å›è°ƒè¾ƒå¤š"}
                    ],
                    "sectors": ["åŠå¯¼ä½“è®¾å¤‡", "åŠå¯¼ä½“ææ–™", "EDA", "å…ˆè¿›å°è£…"],
                    "leading_stocks": [
                        {"name": "åŒ—æ–¹ååˆ›", "logic": "è®¾å¤‡å¹³å°å‹é¾™å¤´ï¼Œäº§å“çº¿æœ€å…¨"},
                        {"name": "ä¸­å¾®å…¬å¸", "logic": "åˆ»èš€è®¾å¤‡é¾™å¤´ï¼ŒæŠ€æœ¯é¢†å…ˆ"},
                        {"name": "åå¤§ä¹å¤©", "logic": "EDAé¾™å¤´ï¼Œå›½äº§åŒ–ç‡æå‡"},
                        {"name": "é•¿ç”µç§‘æŠ€", "logic": "å°æµ‹é¾™å¤´ï¼Œå…ˆè¿›å°è£…å—ç›Š"}
                    ],
                    "duration_weeks": 24,
                    "risks": ["æŠ€æœ¯çªç ´ä¸åŠé¢„æœŸ", "å‘¨æœŸæ³¢åŠ¨", "å®¢æˆ·éªŒè¯å‘¨æœŸé•¿"]
                },
                {
                    "name": "æ–°èƒ½æºè½¬å‹",
                    "score": 72,
                    "confidence": 0.75,
                    "core_logic": "ç¢³ä¸­å’Œç›®æ ‡é©±åŠ¨èƒ½æºç»“æ„è½¬å‹ï¼Œä½†çŸ­æœŸé¢ä¸´äº§èƒ½è¿‡å‰©å‹åŠ›",
                    "logic_chain": [
                        "å‰æ1: å…¨çƒç¢³ä¸­å’Œç›®æ ‡æ˜ç¡®",
                        "å‰æ2: æ–°èƒ½æºæˆæœ¬æŒç»­ä¸‹é™",
                        "é£é™©: äº§èƒ½è¿‡å‰©å¯¼è‡´ä»·æ ¼æˆ˜",
                        "æ¨è®º: é•¿æœŸé€»è¾‘æ¸…æ™°ï¼ŒçŸ­æœŸéœ€ç²¾é€‰ä¸ªè‚¡"
                    ],
                    "sectors": ["å…‰ä¼", "å‚¨èƒ½", "é”‚ç”µæ± ", "æ–°èƒ½æºè½¦"],
                    "leading_stocks": [
                        {"name": "éš†åŸºç»¿èƒ½", "logic": "å…‰ä¼é¾™å¤´ï¼Œæˆæœ¬ä¼˜åŠ¿æ˜æ˜¾"},
                        {"name": "å®å¾·æ—¶ä»£", "logic": "ç”µæ± é¾™å¤´ï¼ŒæŠ€æœ¯é¢†å…ˆ"},
                        {"name": "é˜³å…‰ç”µæº", "logic": "å‚¨èƒ½é¾™å¤´ï¼Œæµ·å¤–å æ¯”é«˜"},
                        {"name": "æ¯”äºšè¿ª", "logic": "æ–°èƒ½æºè½¦é¾™å¤´ï¼Œå‚ç›´æ•´åˆ"}
                    ],
                    "duration_weeks": 12,
                    "risks": ["äº§èƒ½è¿‡å‰©", "ä»·æ ¼æˆ˜æ¿€çƒˆ", "è´¸æ˜“æ‘©æ“¦"]
                }
            ],
            "market_view": (
                "ã€æ ¸å¿ƒç»“è®ºã€‘å½“å‰å¸‚åœºå¤„äºç»“æ„æ€§è¡Œæƒ…ï¼ŒAIå’Œå›½äº§æ›¿ä»£æ˜¯æœ€å¼ºä¸»çº¿ã€‚\n\n"
                "ã€æ“ä½œå»ºè®®ã€‘\n"
                "1. ä»“ä½é…ç½®ï¼šAI(40%) + å›½äº§æ›¿ä»£(30%) + æ–°èƒ½æº(15%) + ç°é‡‘(15%)\n"
                "2. ä¹°å…¥ç­–ç•¥ï¼šé€¢å›è°ƒåˆ†æ‰¹å»ºä»“ï¼Œé¿å…è¿½é«˜\n"
                "3. é£æ§æªæ–½ï¼šå•ä¸€ä¸»çº¿ä»“ä½ä¸è¶…è¿‡40%ï¼Œè®¾ç½®æ­¢æŸçº¿\n"
                "4. åŠ¨æ€è°ƒæ•´ï¼šå…³æ³¨å­£æŠ¥éªŒè¯ï¼ŒåŠæ—¶è°ƒæ•´æŒä»“"
            ),
            "reasoning": (
                f"ã€{self.model_name}æ·±åº¦åˆ†æã€‘\n\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "ç¬¬ä¸€å±‚ï¼šå®è§‚ç¯å¢ƒæ‰«æ\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "â€¢ æ”¿ç­–å‘¨æœŸï¼šå®½æ¾å‹å¥½ï¼Œç§‘æŠ€åˆ›æ–°æ˜¯å›½å®¶æˆ˜ç•¥\n"
                "â€¢ ç»æµå‘¨æœŸï¼šå¼±å¤è‹ï¼Œç»“æ„åˆ†åŒ–ï¼Œç§‘æŠ€æˆé•¿å ä¼˜\n"
                "â€¢ æµåŠ¨æ€§ï¼šå……è£•ï¼Œåˆ©ç‡ä¸‹è¡Œï¼Œåˆ©å¥½æˆé•¿è‚¡\n"
                "â€¢ å¤–éƒ¨ç¯å¢ƒï¼šä¸­ç¾åšå¼ˆæŒç»­ï¼Œå›½äº§æ›¿ä»£å¿…è¦æ€§å¢å¼º\n\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "ç¬¬äºŒå±‚ï¼šèµ„é‡‘æµå‘éªŒè¯\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "â€¢ åŒ—å‘èµ„é‡‘ï¼šè¿ç»­10å‘¨å‡€æµå…¥ç§‘æŠ€æ¿å—ï¼Œç´¯è®¡è¶…500äº¿\n"
                "â€¢ ä¸»åŠ›èµ„é‡‘ï¼šAIã€åŠå¯¼ä½“æŒç»­è·å¾—å¤§å•ä¹°å…¥\n"
                "â€¢ ä¸¤èä½™é¢ï¼šç§‘æŠ€è‚¡èèµ„ä½™é¢åˆ›æ–°é«˜\n"
                "â€¢ ETFç”³è´­ï¼šç§‘æŠ€ç±»ETFä»½é¢å¤§å¹…å¢é•¿\n\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "ç¬¬ä¸‰å±‚ï¼šäº§ä¸šæ™¯æ°”ç¡®è®¤\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "â€¢ AIç®—åŠ›ï¼šGPUä¾›ä¸åº”æ±‚ï¼Œç®—åŠ›ä¸­å¿ƒå»ºè®¾åŠ é€Ÿ\n"
                "â€¢ åŠå¯¼ä½“ï¼šè®¾å¤‡è®¢å•é¥±æ»¡ï¼Œå›½äº§åŒ–ç‡æŒç»­æå‡\n"
                "â€¢ å…‰æ¨¡å—ï¼š800Gäº§å“å¼€å§‹æ”¾é‡ï¼Œéœ€æ±‚è¶…é¢„æœŸ\n\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "ç¬¬å››å±‚ï¼šæŠ€æœ¯å½¢æ€æ”¯æ’‘\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "â€¢ AIæ¿å—ï¼šå‡çº¿å¤šå¤´æ’åˆ—ï¼ŒMACDé‡‘å‰\n"
                "â€¢ åŠå¯¼ä½“ï¼šçªç ´å‰æœŸå¹³å°ï¼Œæ”¾é‡ä¸Šæ¶¨\n"
                "â€¢ é¾™å¤´è‚¡ï¼šå¤šåªä¸ªè‚¡åˆ›å†å²æ–°é«˜\n\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "ç¬¬äº”å±‚ï¼šé£é™©æƒ…æ™¯åˆ†æ\n"
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
                "â€¢ ä¹è§‚æƒ…æ™¯(35%): AIåº”ç”¨çˆ†å‘ï¼Œæ”¶ç›Š50%+\n"
                "â€¢ åŸºå‡†æƒ…æ™¯(45%): æ­£å¸¸å‘å±•ï¼Œæ”¶ç›Š20-30%\n"
                "â€¢ æ‚²è§‚æƒ…æ™¯(20%): ç›‘ç®¡æ”¶ç´§ï¼Œæ”¶ç›Š-10%\n\n"
                "ã€ç»¼åˆåˆ¤æ–­ã€‘\n"
                "å½“å‰å¸‚åœºå…·å¤‡ç»“æ„æ€§æœºä¼šï¼ŒAIå’Œå›½äº§æ›¿ä»£å…·å¤‡\n"
                "æ”¿ç­–+èµ„é‡‘+æ™¯æ°”+æŠ€æœ¯çš„å››é‡å…±æŒ¯ï¼Œæ˜¯æœ€å¼ºä¸»çº¿ã€‚"
            )
        }
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    def _analyze_mainline(self, prompt: str) -> str:
        """æ ‡å‡†ä¸»çº¿åˆ†æ"""
        result = {
            "analysis_model": self.model_name,
            "analysis_depth": "standard",
            "mainlines": [
                {
                    "name": "äººå·¥æ™ºèƒ½é©å‘½",
                    "score": 88,
                    "core_logic": "AIå¤§æ¨¡å‹æŠ€æœ¯çªç ´å¼•å‘æ–°ä¸€è½®ç§‘æŠ€é©å‘½ï¼Œç®—åŠ›ã€åº”ç”¨ã€æ•°æ®å…¨äº§ä¸šé“¾å—ç›Š",
                    "supporting_factors": [
                        "æ”¿ç­–å¼ºåŠ›æ”¯æŒï¼Œçº³å…¥å›½å®¶æˆ˜ç•¥",
                        "åŒ—å‘èµ„é‡‘æŒç»­æµå…¥ç§‘æŠ€æ¿å—",
                        "è¡Œä¸šæ™¯æ°”åº¦é«˜ä¼ï¼Œä¸šç»©å…‘ç°ä¸­",
                        "å…¨çƒAIäº§ä¸šè¶‹åŠ¿æ˜ç¡®"
                    ],
                    "sectors": ["AIç®—åŠ›", "AIåº”ç”¨", "å…‰æ¨¡å—", "æ•°æ®ä¸­å¿ƒ"],
                    "leading_stocks": ["å¯’æ­¦çºª", "ä¸­ç§‘æ›™å…‰", "ç§‘å¤§è®¯é£", "ä¸­é™…æ—­åˆ›"],
                    "duration_weeks": 16,
                    "risks": ["ä¼°å€¼è¾ƒé«˜", "ä¸šç»©å…‘ç°å‹åŠ›", "æµ·å¤–æ”¿ç­–é£é™©"]
                },
                {
                    "name": "å›½äº§æ›¿ä»£åŠ é€Ÿ",
                    "score": 82,
                    "core_logic": "å¤–éƒ¨å‹åŠ›å€’é€¼å›½äº§åŒ–è¿›ç¨‹ï¼ŒåŠå¯¼ä½“è®¾å¤‡ææ–™ã€å·¥ä¸šè½¯ä»¶ç­‰é¢†åŸŸè¿æ¥å†å²æ€§æœºé‡",
                    "supporting_factors": [
                        "æ”¿ç­–å¤§åŠ›æ‰¶æŒï¼Œå¤§åŸºé‡‘æŒç»­æŠ•å…¥",
                        "ä¸‹æ¸¸éœ€æ±‚æ—ºç››ï¼Œè®¢å•é¥±æ»¡",
                        "æŠ€æœ¯çªç ´åŠ é€Ÿï¼Œå›½äº§åŒ–ç‡æå‡",
                        "äº§ä¸šé“¾å®‰å…¨æˆä¸ºå›½å®¶æˆ˜ç•¥"
                    ],
                    "sectors": ["åŠå¯¼ä½“è®¾å¤‡", "åŠå¯¼ä½“ææ–™", "EDA", "å…ˆè¿›å°è£…"],
                    "leading_stocks": ["åŒ—æ–¹ååˆ›", "ä¸­å¾®å…¬å¸", "åå¤§ä¹å¤©", "é•¿ç”µç§‘æŠ€"],
                    "duration_weeks": 24,
                    "risks": ["æŠ€æœ¯çªç ´ä¸åŠé¢„æœŸ", "å‘¨æœŸæ³¢åŠ¨"]
                },
                {
                    "name": "æ–°èƒ½æºè½¬å‹",
                    "score": 72,
                    "core_logic": "ç¢³ä¸­å’Œç›®æ ‡é©±åŠ¨èƒ½æºç»“æ„è½¬å‹ï¼Œå…‰ä¼å‚¨èƒ½æ–°èƒ½æºè½¦æŒç»­æ¸—é€",
                    "supporting_factors": [
                        "æ”¿ç­–ç›®æ ‡æ˜ç¡®ï¼Œé•¿æœŸé€»è¾‘æ¸…æ™°",
                        "æˆæœ¬æŒç»­ä¸‹é™ï¼Œç«äº‰åŠ›å¢å¼º",
                        "æµ·å¤–éœ€æ±‚æ—ºç››ï¼Œå‡ºå£é«˜å¢"
                    ],
                    "sectors": ["å…‰ä¼", "å‚¨èƒ½", "é”‚ç”µæ± ", "æ–°èƒ½æºè½¦"],
                    "leading_stocks": ["éš†åŸºç»¿èƒ½", "å®å¾·æ—¶ä»£", "é˜³å…‰ç”µæº", "æ¯”äºšè¿ª"],
                    "duration_weeks": 12,
                    "risks": ["äº§èƒ½è¿‡å‰©", "ä»·æ ¼æˆ˜", "è´¸æ˜“æ‘©æ“¦"]
                }
            ],
            "market_view": (
                "å½“å‰å¸‚åœºå¤„äºç»“æ„æ€§è¡Œæƒ…ï¼Œç§‘æŠ€æˆé•¿æ˜¯æ ¸å¿ƒä¸»çº¿ã€‚"
                "å»ºè®®é‡ç‚¹å…³æ³¨AIå’Œå›½äº§æ›¿ä»£ä¸¤å¤§æ–¹å‘ï¼ŒåŒæ—¶å…³æ³¨æ–°èƒ½æºçš„é˜¶æ®µæ€§æœºä¼šã€‚"
            ),
            "reasoning": (
                f"ã€{self.model_name}åˆ†æã€‘\n\n"
                "ã€å®è§‚ç¯å¢ƒã€‘æ”¿ç­–å®½æ¾ï¼Œæ”¯æŒç§‘æŠ€åˆ›æ–°\n"
                "ã€èµ„é‡‘æµå‘ã€‘åŒ—å‘èµ„é‡‘å‡€æµå…¥ç§‘æŠ€\n"
                "ã€è¡Œä¸šæ™¯æ°”ã€‘AIç®—åŠ›ã€åŠå¯¼ä½“é«˜æ™¯æ°”\n"
                "ã€æŠ€æœ¯å½¢æ€ã€‘å¼ºåŠ¿æ¿å—å‡çº¿å¤šå¤´\n"
                "ã€ç»¼åˆåˆ¤æ–­ã€‘AIå’Œå›½äº§æ›¿ä»£æ˜¯æœ€å¼ºä¸»çº¿"
            )
        }
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    def _analyze_research(self, prompt: str) -> str:
        """ç ”æŠ¥åˆ†æ"""
        result = {
            "core_view": "çœ‹å¥½è¡Œä¸šé•¿æœŸå‘å±•ï¼Œç»´æŒæ¨èè¯„çº§",
            "target_price": "N/A",
            "rating": "æ¨è",
            "key_assumptions": [
                "è¡Œä¸šéœ€æ±‚æŒç»­å¢é•¿",
                "å…¬å¸å¸‚åœºä»½é¢æå‡",
                "æ¯›åˆ©ç‡ä¿æŒç¨³å®š"
            ],
            "unique_insights": [
                "å…³æ³¨ç»†åˆ†é¢†åŸŸé¾™å¤´çš„ç«äº‰ä¼˜åŠ¿",
                "æŠ€æœ¯è¿­ä»£å¸¦æ¥çš„ä¼°å€¼é‡å¡‘æœºä¼š"
            ],
            "vs_consensus": "ç•¥é«˜äºå¸‚åœºé¢„æœŸ",
            "credibility": 75,
            "reasoning": "ç ”æŠ¥é€»è¾‘æ¸…æ™°ï¼Œæ•°æ®æ”¯æ’‘å……åˆ†ï¼Œä½†éœ€å…³æ³¨å‡è®¾çš„å®ç°æƒ…å†µ"
        }
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    def _general_analysis(self, prompt: str) -> str:
        """é€šç”¨åˆ†æ"""
        return json.dumps({
            "analysis": "åˆ†æå®Œæˆ",
            "reasoning": "åŸºäºä¸“ä¸šçŸ¥è¯†åº“è¿›è¡Œç»¼åˆåˆ†æ"
        }, ensure_ascii=False, indent=2)


class LLMAnalyzer:
    """
    LLMåˆ†æå™¨
    
    æ”¯æŒCursorå†…ç½®çš„å¤šä¸ªé¡¶çº§æ¨¡å‹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  æ¨¡å‹              â”‚ æ¨ç†èƒ½åŠ› â”‚ é€Ÿåº¦ â”‚ æ¨èåœºæ™¯    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Claude Opus 4     â”‚  â˜…â˜…â˜…â˜…â˜…  â”‚ â˜…â˜…â˜…  â”‚ å¤æ‚ç­–ç•¥    â”‚
    â”‚  GPT-4o            â”‚  â˜…â˜…â˜…â˜…â˜†  â”‚ â˜…â˜…â˜…â˜… â”‚ æ—¥å¸¸åˆ†æ    â”‚
    â”‚  Gemini 2.5 Pro    â”‚  â˜…â˜…â˜…â˜…â˜†  â”‚ â˜…â˜…â˜…â˜† â”‚ é•¿æ–‡æ¡£      â”‚
    â”‚  Claude Sonnet 4   â”‚  â˜…â˜…â˜…â˜…â˜†  â”‚ â˜…â˜…â˜…â˜… â”‚ é«˜æ€§ä»·æ¯”    â”‚
    â”‚  o3-mini           â”‚  â˜…â˜…â˜…â˜†â˜†  â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ å¿«é€Ÿæ‰«æ    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    def __init__(
        self, 
        config: Optional[LLMConfig] = None,
        cursor_model: CursorModel = CursorModel.CLAUDE_OPUS_4  # é»˜è®¤ä½¿ç”¨æœ€å¼ºæ¨¡å‹
    ):
        self.config = config or self._get_default_config(cursor_model)
        self._client = None
        self._cursor_client = CursorLLMClient(cursor_model)
        self._init_client()
    
    def _get_default_config(self, cursor_model: CursorModel) -> LLMConfig:
        """è·å–é»˜è®¤é…ç½® - ä½¿ç”¨Cursorå†…ç½®æ¨¡å‹"""
        return LLMConfig(
            provider=LLMProvider.CURSOR,
            model=cursor_model.value,
            cursor_model=cursor_model,
        )
    
    def set_model(self, model: CursorModel):
        """åˆ‡æ¢Cursoræ¨¡å‹"""
        self._cursor_client.set_model(model)
        self.config.cursor_model = model
        self.config.model = model.value
        logger.info(f"ğŸ”„ LLMåˆ†æå™¨åˆ‡æ¢åˆ°: {model.value}")
    
    def _init_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            if self.config.provider == LLMProvider.CURSOR:
                self._client = self._cursor_client
                logger.info(f"âœ… ä½¿ç”¨Cursorå†…ç½®æ¨¡å‹: {self.config.model}")
            elif self.config.provider == LLMProvider.OPENAI:
                if os.getenv("OPENAI_API_KEY"):
                    from openai import OpenAI
                    self._client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                    logger.info("âœ… ä½¿ç”¨OpenAIæ¨¡å‹")
            elif self.config.provider == LLMProvider.ANTHROPIC:
                if os.getenv("ANTHROPIC_API_KEY"):
                    from anthropic import Anthropic
                    self._client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
                    logger.info("âœ… ä½¿ç”¨Anthropicæ¨¡å‹")
            elif self.config.provider == LLMProvider.OLLAMA:
                self._client = "ollama"
                logger.info("âœ… ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹")
            
            if self._client is None and self.config.provider != LLMProvider.CURSOR:
                # å›é€€åˆ°Cursor
                self._client = self._cursor_client
                self.config.provider = LLMProvider.CURSOR
                logger.info("âš ï¸ å›é€€åˆ°Cursorå†…ç½®æ¨¡å‹")
                
        except Exception as e:
            logger.warning(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨Cursorå†…ç½®æ¨¡å‹")
            self._client = self._cursor_client
            self.config.provider = LLMProvider.CURSOR
    
    def analyze_policy(self, policy_data: Dict) -> AnalysisResult:
        """åˆ†ææ”¿ç­–"""
        prompt = PROMPTS["policy_analysis"].format(
            policy_data=json.dumps(policy_data, ensure_ascii=False, indent=2)
        )
        return self._run_analysis("policy_analysis", policy_data, prompt)
    
    def analyze_industry(self, industry_data: Dict) -> AnalysisResult:
        """åˆ†æè¡Œä¸šæ™¯æ°”åº¦"""
        prompt = PROMPTS["industry_analysis"].format(
            industry_data=json.dumps(industry_data, ensure_ascii=False, indent=2)
        )
        return self._run_analysis("industry_analysis", industry_data, prompt)
    
    def synthesize_mainline(
        self,
        macro_data: Dict,
        capital_data: Dict,
        industry_data: Dict,
        technical_data: Dict,
    ) -> AnalysisResult:
        """ç»¼åˆè¯†åˆ«ä¸»çº¿"""
        prompt = PROMPTS["mainline_synthesis"].format(
            macro_data=json.dumps(macro_data, ensure_ascii=False, indent=2),
            capital_data=json.dumps(capital_data, ensure_ascii=False, indent=2),
            industry_data=json.dumps(industry_data, ensure_ascii=False, indent=2),
            technical_data=json.dumps(technical_data, ensure_ascii=False, indent=2),
        )
        
        input_data = {
            "macro": macro_data,
            "capital": capital_data,
            "industry": industry_data,
            "technical": technical_data,
        }
        
        return self._run_analysis("mainline_synthesis", input_data, prompt)
    
    def summarize_research(self, research_content: str) -> AnalysisResult:
        """æ€»ç»“ç ”æŠ¥"""
        prompt = PROMPTS["research_summary"].format(
            research_content=research_content
        )
        return self._run_analysis("research_summary", {"content": research_content[:500]}, prompt)
    
    def _run_analysis(
        self,
        task: str,
        input_data: Dict,
        prompt: str
    ) -> AnalysisResult:
        """æ‰§è¡Œåˆ†æ"""
        start_time = datetime.now()
        
        try:
            if self.config.provider == LLMProvider.CURSOR:
                response = self._cursor_client.analyze(prompt)
            elif self.config.provider == LLMProvider.OPENAI:
                response = self._call_openai(prompt)
            elif self.config.provider == LLMProvider.ANTHROPIC:
                response = self._call_anthropic(prompt)
            elif self.config.provider == LLMProvider.OLLAMA:
                response = self._call_ollama(prompt)
            else:
                response = self._cursor_client.analyze(prompt)
            
            # è§£æå“åº”
            output, reasoning = self._parse_response(response)
            
            return AnalysisResult(
                task=task,
                input_data=input_data,
                output=output,
                reasoning=reasoning,
                confidence=0.85,
                sources_used=list(input_data.keys()),
                model_used=f"{self.config.provider.value}/{self.config.model}",
                tokens_used=len(prompt) + len(response),
                analysis_time=datetime.now(),
            )
            
        except Exception as e:
            logger.error(f"LLMåˆ†æå¤±è´¥: {e}")
            # ä½¿ç”¨Cursorä½œä¸ºåå¤‡
            response = self._cursor_client.analyze(prompt)
            output, reasoning = self._parse_response(response)
            
            return AnalysisResult(
                task=task,
                input_data=input_data,
                output=output,
                reasoning=reasoning,
                confidence=0.75,
                sources_used=list(input_data.keys()),
                model_used="cursor/fallback",
                tokens_used=0,
                analysis_time=datetime.now(),
            )
    
    def _call_openai(self, prompt: str) -> str:
        """è°ƒç”¨OpenAI API"""
        response = self._client.chat.completions.create(
            model=self.config.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )
        return response.choices[0].message.content
    
    def _call_anthropic(self, prompt: str) -> str:
        """è°ƒç”¨Anthropic API"""
        response = self._client.messages.create(
            model=self.config.model,
            max_tokens=self.config.max_tokens,
            messages=[{"role": "user", "content": prompt}],
        )
        return response.content[0].text
    
    def _call_ollama(self, prompt: str) -> str:
        """è°ƒç”¨Ollama API"""
        import requests
        
        base_url = self.config.base_url or "http://localhost:11434"
        response = requests.post(
            f"{base_url}/api/generate",
            json={
                "model": self.config.model or "qwen2:7b",
                "prompt": prompt,
                "stream": False,
            },
            timeout=60,
        )
        return response.json().get("response", "")
    
    def _parse_response(self, response: str) -> tuple:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•æå–JSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())
                output = json.dumps(data, ensure_ascii=False, indent=2)
                reasoning = data.get("reasoning", "")
                return output, reasoning
        except:
            pass
        
        return response, ""
    
    def get_available_providers(self) -> List[Dict]:
        """è·å–å¯ç”¨çš„LLMæä¾›å•†"""
        providers = []
        
        # Cursorå†…ç½®æ¨¡å‹ï¼ˆé¦–é€‰ï¼‰
        for model in CursorModel:
            info = CURSOR_MODEL_CAPABILITIES.get(model, {})
            providers.append({
                "id": f"cursor/{model.value}",
                "name": info.get("name", model.value),
                "provider": f"Cursor ({info.get('provider', 'Unknown')})",
                "model": model.value,
                "status": "available",
                "reasoning": info.get("reasoning", 0),
                "speed": info.get("speed", 0),
                "cost": info.get("cost", "æœªçŸ¥"),
                "best_for": info.get("best_for", []),
                "description": info.get("description", ""),
                "is_cursor": True,
            })
        
        # å¤–éƒ¨APIï¼ˆå¤‡é€‰ï¼‰
        if os.getenv("OPENAI_API_KEY"):
            providers.append({
                "id": "openai/gpt-4-turbo",
                "name": "OpenAI GPT-4 Turbo",
                "provider": "OpenAI API",
                "model": "gpt-4-turbo",
                "status": "available",
                "description": "éœ€è¦APIå¯†é’¥",
                "is_cursor": False,
            })
        
        if os.getenv("ANTHROPIC_API_KEY"):
            providers.append({
                "id": "anthropic/claude-3-opus",
                "name": "Anthropic Claude 3 Opus",
                "provider": "Anthropic API",
                "model": "claude-3-opus",
                "status": "available",
                "description": "éœ€è¦APIå¯†é’¥",
                "is_cursor": False,
            })
        
        # æœ¬åœ°Ollama
        try:
            import requests
            resp = requests.get("http://localhost:11434/api/tags", timeout=2)
            if resp.status_code == 200:
                providers.append({
                    "id": "ollama/qwen2",
                    "name": "æœ¬åœ°Ollama",
                    "provider": "æœ¬åœ°",
                    "model": "qwen2:7b",
                    "status": "available",
                    "description": "æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ç½‘ç»œ",
                    "is_cursor": False,
                })
        except:
            pass
        
        return providers
    
    def get_current_model_info(self) -> Dict:
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯"""
        model = self.config.cursor_model or CursorModel.CLAUDE_OPUS_4
        info = CURSOR_MODEL_CAPABILITIES.get(model, {})
        return {
            "model": model.value,
            "name": info.get("name", model.value),
            "provider": info.get("provider", "Unknown"),
            "reasoning": info.get("reasoning", 0),
            "speed": info.get("speed", 0),
            "cost": info.get("cost", "æœªçŸ¥"),
            "best_for": info.get("best_for", []),
            "description": info.get("description", ""),
        }


# å…¨å±€å®ä¾‹ - é»˜è®¤ä½¿ç”¨Claude Opus 4ï¼ˆæœ€å¼ºæ¨ç†ï¼‰
llm_analyzer = LLMAnalyzer(cursor_model=CursorModel.CLAUDE_OPUS_4)
